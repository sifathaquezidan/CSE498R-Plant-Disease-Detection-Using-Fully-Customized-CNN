{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13166744,"sourceType":"datasetVersion","datasetId":8343159}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install thop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 1 — Setup + Data + Model\n# =========================\n\nimport os, time, copy, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n\n# FLOPs (thop)\ntry:\n    from thop import profile\n    THOP_AVAILABLE = True\nexcept Exception:\n    THOP_AVAILABLE = False\n    print(\"thop not available — FLOPs will be reported as N/A\")\n\n\n# Settings\nDATA_ROOT = \"/kaggle/input/plant-disease-dataset/Dataset_Final_V2_Split\"\nOUT_DIR = \"/kaggle/working/plantanet_relu_final\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nIMG_SIZE = 160\nBATCH_SIZE = 32\nEPOCHS = 100\n\n# Config 3 (best from tuning)\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nMIXUP_ALPHA = 0.2\n\nNUM_WORKERS = 4\nSEED = 42\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {DEVICE} | CUDA: {torch.cuda.is_available()} | GPUs: {torch.cuda.device_count()}\")\n\n\n# Reproducibility\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n\n# Transforms\ntrain_tfm = transforms.Compose([\n    transforms.RandomResizedCrop(\n        IMG_SIZE,\n        scale=(0.9, 1.0),\n        ratio=(0.95, 1.05)\n    ),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(degrees=10),\n    transforms.ColorJitter(\n        brightness=0.15,\n        contrast=0.15,\n        saturation=0.15\n    ),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\neval_tfm = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    ),\n])\n\n\n# Datasets & Dataloaders\ntrain_dir = os.path.join(DATA_ROOT, \"train\")\nval_dir   = os.path.join(DATA_ROOT, \"val\")\ntest_dir  = os.path.join(DATA_ROOT, \"test\")\n\nassert os.path.exists(train_dir) and os.path.exists(val_dir), \\\n    f\"Dataset folders not found under {DATA_ROOT}. Expected train/ val/.\"\n\ntrain_ds = datasets.ImageFolder(train_dir, transform=train_tfm)\nval_ds   = datasets.ImageFolder(val_dir,   transform=eval_tfm)\n\ntest_ds = datasets.ImageFolder(test_dir, transform=eval_tfm) if os.path.exists(test_dir) else None\n\ntrain_loader = DataLoader(\n    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\nval_loader = DataLoader(\n    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\ntest_loader = None\nif test_ds is not None:\n    test_loader = DataLoader(\n        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n        num_workers=NUM_WORKERS, pin_memory=True\n    )\n\nclass_names = train_ds.classes\nNUM_CLASSES = len(class_names)\nprint(f\"Classes={NUM_CLASSES} | Train={len(train_ds)}, Val={len(val_ds)}, Test={(len(test_ds) if test_ds else 0)}\")\n\n\n# Model definition — PlantaNet-ReLU\nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        self.depth = nn.Conv2d(in_ch, in_ch, 3, stride, 1, groups=in_ch, bias=False)\n        self.point = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n        self.norm = nn.GroupNorm(8 if out_ch % 8 == 0 else 4, out_ch)\n        self.act = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.depth(x)\n        x = self.point(x)\n        x = self.norm(x)\n        x = self.act(x)\n        return x\n\n\nclass PlantaNetReLU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        c1, c2, c3, c4 = 144, 224, 320, 448\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(3, c1, 3, 1, 1, bias=False),\n            nn.GroupNorm(8 if c1 % 8 == 0 else 4, c1),\n            nn.ReLU(inplace=True),\n        )\n\n        self.block1 = nn.Sequential(\n            DepthwiseSeparableConv(c1, c2, stride=2),\n            nn.Dropout(0.15)\n        )\n        self.block2 = nn.Sequential(\n            DepthwiseSeparableConv(c2, c3, stride=2),\n            nn.Dropout(0.20)\n        )\n        self.block3 = nn.Sequential(\n            DepthwiseSeparableConv(c3, c4, stride=2),\n            nn.Dropout(0.25)\n        )\n\n        self.conv_extra = nn.Sequential(\n            nn.Conv2d(c4, c4, 3, 1, 1, bias=False),\n            nn.GroupNorm(8 if c4 % 8 == 0 else 4, c4),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.25),\n        )\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(c4, 1024, bias=False),\n            nn.GroupNorm(8 if 1024 % 8 == 0 else 4, 1024),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.4),\n            nn.Linear(1024, num_classes),\n        )\n\n    def forward(self, x):\n        x = self.stem(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.conv_extra(x)\n        x = self.gap(x)\n        x = self.classifier(x)\n        return x\n\n\ndef count_parameters(model: nn.Module) -> int:\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# Instantiate model & efficiency metrics\nmodel = PlantaNetReLU(NUM_CLASSES)\nparams = count_parameters(model)\nprint(f\"Model params: {params:,} ({params/1e6:.3f} M)\")\n\n# FLOPs\nflops_M = None\nif THOP_AVAILABLE:\n    try:\n        model_for_flops = PlantaNetReLU(NUM_CLASSES)\n        dummy_cpu = torch.randn(1, 3, IMG_SIZE, IMG_SIZE)\n        f, _ = profile(model_for_flops, inputs=(dummy_cpu,), verbose=False)\n        flops_M = f / 1e6\n    except Exception as e:\n        print(\"FLOPs profiling failed:\", e)\n\n# Move model to device and DataParallel if possible\nmodel = model.to(DEVICE)\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel.\")\n    model = nn.DataParallel(model)\n\n# Inference time and model size\nmodel.eval()\ndummy = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(DEVICE)\nif DEVICE.type == \"cuda\":\n    torch.cuda.synchronize()\nstart = time.time()\nwith torch.no_grad():\n    for _ in range(30):\n        _ = model(dummy)\nif DEVICE.type == \"cuda\":\n    torch.cuda.synchronize()\ninf_ms = (time.time() - start) / 30 * 1000.0\n\ntmp_path = os.path.join(OUT_DIR, \"PlantaNet_ReLU_tmp.pth\")\ntorch.save(model.state_dict(), tmp_path)\nsize_mb = os.path.getsize(tmp_path) / (1024 * 1024)\nos.remove(tmp_path)\n\nprint(f\"FLOPs={(round(flops_M,2) if flops_M else 'N/A')}M | Inference={inf_ms:.2f} ms | Size={size_mb:.2f} MB\")\n\n\n# Loss & optimizer (Config 3)\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\noptimizer = optim.Adamax(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T13:57:50.781264Z","iopub.execute_input":"2025-12-01T13:57:50.781549Z","iopub.status.idle":"2025-12-01T14:00:28.490586Z","shell.execute_reply.started":"2025-12-01T13:57:50.781528Z","shell.execute_reply":"2025-12-01T14:00:28.489919Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | CUDA: True | GPUs: 2\nClasses=51 | Train=95504, Val=20472, Test=20506\nModel params: 2,579,955 (2.580 M)\nUsing 2 GPUs with DataParallel.\nFLOPs=1213.9M | Inference=23.48 ms | Size=9.85 MB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================\n# Cell 2 — MixUp + Training + Test + Plots\n# =========================\n\ndef mixup_data(x, y, alpha=MIXUP_ALPHA):\n    \"\"\"Returns mixed inputs, pairs of targets, and lambda.\"\"\"\n    if alpha <= 0:\n        return x, y, y, 1.0\n    lam = np.random.beta(alpha, alpha)\n    batch_size = x.size(0)\n    index = torch.randperm(batch_size).to(x.device)\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, preds, y_a, y_b, lam):\n    return lam * criterion(preds, y_a) + (1 - lam) * criterion(preds, y_b)\n\n\ndef train_one_epoch(loader, mixup_alpha=MIXUP_ALPHA):\n    model.train()\n    running_loss = 0.0\n    total = 0\n    start_time = time.time()\n\n    for inputs, labels in loader:\n        inputs = inputs.to(DEVICE, non_blocking=True)\n        labels = labels.to(DEVICE, non_blocking=True)\n        optimizer.zero_grad()\n\n        inputs_m, y_a, y_b, lam = mixup_data(inputs, labels, alpha=mixup_alpha)\n        outputs = model(inputs_m)\n        loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n\n        loss.backward()\n        optimizer.step()\n\n        bs = inputs.size(0)\n        running_loss += loss.item() * bs\n        total += bs\n\n    return running_loss / total, time.time() - start_time\n\n\ndef eval_one_epoch(loader):\n    model.eval()\n    running_loss = 0.0\n    running_correct = 0\n    total = 0\n    preds_all, labels_all = [], []\n    start_time = time.time()\n\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(DEVICE, non_blocking=True)\n            labels = labels.to(DEVICE, non_blocking=True)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            _, preds = torch.max(outputs, 1)\n\n            bs = inputs.size(0)\n            running_loss += loss.item() * bs\n            running_correct += torch.sum(preds == labels).item()\n            total += bs\n\n            preds_all.append(preds.cpu().numpy())\n            labels_all.append(labels.cpu().numpy())\n\n    preds_all = np.concatenate(preds_all) if preds_all else np.array([])\n    labels_all = np.concatenate(labels_all) if labels_all else np.array([])\n    return running_loss / total, running_correct / total, preds_all, labels_all, time.time() - start_time\n\n\n# Main training loop\nhistory = []\nbest_val_acc = 0.0\nbest_state = None\nt0 = time.time()\n\nprint(\"\\nTraining starts:\")\nfor epoch in range(1, EPOCHS + 1):\n    _mixup_loss, tr_time = train_one_epoch(train_loader, mixup_alpha=MIXUP_ALPHA)\n    train_loss_clean, train_acc_clean, _, _, tr_eval_time = eval_one_epoch(train_loader)\n    val_loss, val_acc, val_preds, val_labels, val_time = eval_one_epoch(val_loader)\n\n    epoch_time = tr_time + tr_eval_time + val_time\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        best_state = copy.deepcopy(\n            model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n        )\n        torch.save(best_state, os.path.join(OUT_DIR, \"PlantaNet_ReLU_best_weights.pth\"))\n\n    history.append({\n        \"epoch\": epoch,\n        \"train_loss\": float(train_loss_clean),\n        \"train_acc\": float(train_acc_clean),\n        \"val_loss\": float(val_loss),\n        \"val_acc\": float(val_acc),\n        \"epoch_time_sec\": float(epoch_time),\n    })\n\n    print(\n        f\"Epoch {epoch:03d}/{EPOCHS} | \"\n        f\"train_loss={train_loss_clean:.4f} | train_acc={train_acc_clean:.4f} | \"\n        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | \"\n        f\"time={epoch_time:.1f}s\"\n    )\n\nprint(f\"\\nTotal training time: {(time.time()-t0)/60:.2f} minutes\")\n\nhist_df = pd.DataFrame(history)\nhist_df.to_csv(os.path.join(OUT_DIR, \"PlantaNet_ReLU_history.csv\"), index=False)\n\n\n# Evaluate best model on test (if exists)\nbest_weights_path = os.path.join(OUT_DIR, \"PlantaNet_ReLU_best_weights.pth\")\n\nif best_state is not None and os.path.exists(best_weights_path):\n    eval_model = PlantaNetReLU(NUM_CLASSES)\n    eval_model.load_state_dict(torch.load(best_weights_path, map_location=DEVICE), strict=False)\n    eval_model = eval_model.to(DEVICE)\nelse:\n    eval_model = model.module if isinstance(model, nn.DataParallel) else model\n    eval_model = eval_model.to(DEVICE)\n\ntest_acc, prec, rec, f1 = None, None, None, None\ny_true, y_pred = None, None\n\nif test_loader is not None:\n    eval_model.eval()\n    preds_list, labels_list = [], []\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs = inputs.to(DEVICE, non_blocking=True)\n            labels = labels.to(DEVICE, non_blocking=True)\n            outputs = eval_model(inputs)\n            _, preds = torch.max(outputs, 1)\n            preds_list.append(preds.cpu().numpy())\n            labels_list.append(labels.cpu().numpy())\n\n    y_pred = np.concatenate(preds_list)\n    y_true = np.concatenate(labels_list)\n\n    test_acc = float((y_pred == y_true).mean())\n    prec, rec, f1, _ = precision_recall_fscore_support(\n        y_true, y_pred, average=\"weighted\", zero_division=0\n    )\n\n    print(\n        f\"\\nTest results: TestAcc={test_acc:.4f}, \"\n        f\"Precision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}\"\n    )\n\n    report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True, zero_division=0)\n    pd.DataFrame(report).T.to_csv(os.path.join(OUT_DIR, \"classification_report.csv\"), index=False)\n\ntorch.save(eval_model.state_dict(), os.path.join(OUT_DIR, \"PlantaNet_ReLU_final_weights.pth\"))\n\n\n# Plots\nplt.figure(figsize=(8, 5))\nplt.plot(hist_df[\"epoch\"], hist_df[\"train_loss\"], label=\"Train Loss\")\nplt.plot(hist_df[\"epoch\"], hist_df[\"val_loss\"], label=\"Val Loss\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.legend()\nplt.title(\"Loss Curve\"); plt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"loss_curve.png\"), dpi=150)\nplt.close()\n\nplt.figure(figsize=(8, 5))\nplt.plot(hist_df[\"epoch\"], hist_df[\"train_acc\"], label=\"Train Acc\")\nplt.plot(hist_df[\"epoch\"], hist_df[\"val_acc\"], label=\"Val Acc\")\nplt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.legend()\nplt.title(\"Accuracy Curve\"); plt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"accuracy_curve.png\"), dpi=150)\nplt.close()\n\n\n# Confusion matrix if test exists\nif y_true is not None:\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(12, 10))\n    sns.heatmap(cm, annot=False, cmap=\"Blues\")\n    plt.title(\"Confusion Matrix - PlantaNet_ReLU\")\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUT_DIR, \"confusion_matrix.png\"), dpi=150)\n    plt.close()\n\n\n# Efficiency summary\nsummary = pd.DataFrame([{\n    \"Model\": \"PlantaNet_ReLU\",\n    \"Params(M)\": round(params / 1e6, 3),\n    \"FLOPs(M)\": None if flops_M is None else round(flops_M, 2),\n    \"Inference(ms)\": round(inf_ms, 2),\n    \"Size(MB)\": round(size_mb, 2),\n    \"ValAcc(%)\": round(best_val_acc * 100, 2),\n    \"TestAcc(%)\": None if test_acc is None else round(test_acc * 100, 2),\n    \"Precision\": None if prec is None else round(prec, 3),\n    \"Recall\": None if rec is None else round(rec, 3),\n    \"F1\": None if f1 is None else round(f1, 3),\n}])\n\nsummary.to_csv(os.path.join(OUT_DIR, \"PlantaNet_ReLU_efficiency_summary.csv\"), index=False)\nprint(\"\\nEfficiency summary:\")\nprint(summary.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T14:01:29.080133Z","iopub.execute_input":"2025-12-01T14:01:29.080439Z","iopub.status.idle":"2025-12-02T01:40:34.651169Z","shell.execute_reply.started":"2025-12-01T14:01:29.080417Z","shell.execute_reply":"2025-12-02T01:40:34.650420Z"}},"outputs":[{"name":"stdout","text":"\nTraining starts:\nEpoch 001/100 | train_loss=1.3664 | train_acc=0.7744 | val_loss=1.3814 | val_acc=0.7649 | time=437.8s\nEpoch 002/100 | train_loss=1.1221 | train_acc=0.8705 | val_loss=1.1191 | val_acc=0.8704 | time=413.3s\nEpoch 003/100 | train_loss=0.9774 | train_acc=0.9250 | val_loss=0.9703 | val_acc=0.9264 | time=408.5s\nEpoch 004/100 | train_loss=0.9243 | train_acc=0.9401 | val_loss=0.9203 | val_acc=0.9399 | time=412.0s\nEpoch 005/100 | train_loss=0.8856 | train_acc=0.9558 | val_loss=0.8870 | val_acc=0.9526 | time=412.2s\nEpoch 006/100 | train_loss=0.8532 | train_acc=0.9636 | val_loss=0.8536 | val_acc=0.9625 | time=410.7s\nEpoch 007/100 | train_loss=0.8481 | train_acc=0.9657 | val_loss=0.8550 | val_acc=0.9630 | time=415.4s\nEpoch 008/100 | train_loss=0.8277 | train_acc=0.9679 | val_loss=0.8233 | val_acc=0.9697 | time=413.3s\nEpoch 009/100 | train_loss=0.8033 | train_acc=0.9806 | val_loss=0.8077 | val_acc=0.9789 | time=417.4s\nEpoch 010/100 | train_loss=0.7935 | train_acc=0.9817 | val_loss=0.7993 | val_acc=0.9791 | time=415.8s\nEpoch 011/100 | train_loss=0.7966 | train_acc=0.9809 | val_loss=0.8072 | val_acc=0.9761 | time=414.5s\nEpoch 012/100 | train_loss=0.7924 | train_acc=0.9812 | val_loss=0.7977 | val_acc=0.9783 | time=416.4s\nEpoch 013/100 | train_loss=0.7748 | train_acc=0.9863 | val_loss=0.7796 | val_acc=0.9839 | time=425.2s\nEpoch 014/100 | train_loss=0.7701 | train_acc=0.9863 | val_loss=0.7714 | val_acc=0.9868 | time=414.1s\nEpoch 015/100 | train_loss=0.7592 | train_acc=0.9897 | val_loss=0.7653 | val_acc=0.9883 | time=414.9s\nEpoch 016/100 | train_loss=0.7615 | train_acc=0.9876 | val_loss=0.7669 | val_acc=0.9858 | time=413.8s\nEpoch 017/100 | train_loss=0.7645 | train_acc=0.9872 | val_loss=0.7719 | val_acc=0.9842 | time=436.4s\nEpoch 018/100 | train_loss=0.7515 | train_acc=0.9906 | val_loss=0.7592 | val_acc=0.9883 | time=420.5s\nEpoch 019/100 | train_loss=0.7518 | train_acc=0.9901 | val_loss=0.7575 | val_acc=0.9881 | time=424.5s\nEpoch 020/100 | train_loss=0.7436 | train_acc=0.9926 | val_loss=0.7507 | val_acc=0.9906 | time=418.7s\nEpoch 021/100 | train_loss=0.7486 | train_acc=0.9896 | val_loss=0.7587 | val_acc=0.9870 | time=417.2s\nEpoch 022/100 | train_loss=0.7396 | train_acc=0.9933 | val_loss=0.7491 | val_acc=0.9900 | time=425.3s\nEpoch 023/100 | train_loss=0.7351 | train_acc=0.9944 | val_loss=0.7429 | val_acc=0.9916 | time=437.1s\nEpoch 024/100 | train_loss=0.7363 | train_acc=0.9930 | val_loss=0.7400 | val_acc=0.9920 | time=413.1s\nEpoch 025/100 | train_loss=0.7330 | train_acc=0.9949 | val_loss=0.7402 | val_acc=0.9918 | time=415.5s\nEpoch 026/100 | train_loss=0.7412 | train_acc=0.9911 | val_loss=0.7533 | val_acc=0.9870 | time=451.2s\nEpoch 027/100 | train_loss=0.7378 | train_acc=0.9929 | val_loss=0.7483 | val_acc=0.9886 | time=421.1s\nEpoch 028/100 | train_loss=0.7311 | train_acc=0.9950 | val_loss=0.7386 | val_acc=0.9925 | time=415.0s\nEpoch 029/100 | train_loss=0.7347 | train_acc=0.9938 | val_loss=0.7454 | val_acc=0.9899 | time=418.9s\nEpoch 030/100 | train_loss=0.7366 | train_acc=0.9934 | val_loss=0.7435 | val_acc=0.9912 | time=415.6s\nEpoch 031/100 | train_loss=0.7372 | train_acc=0.9922 | val_loss=0.7484 | val_acc=0.9881 | time=416.4s\nEpoch 032/100 | train_loss=0.7285 | train_acc=0.9956 | val_loss=0.7346 | val_acc=0.9937 | time=441.6s\nEpoch 033/100 | train_loss=0.7254 | train_acc=0.9965 | val_loss=0.7308 | val_acc=0.9943 | time=428.7s\nEpoch 034/100 | train_loss=0.7304 | train_acc=0.9959 | val_loss=0.7358 | val_acc=0.9937 | time=413.7s\nEpoch 035/100 | train_loss=0.7275 | train_acc=0.9966 | val_loss=0.7340 | val_acc=0.9936 | time=418.8s\nEpoch 036/100 | train_loss=0.7285 | train_acc=0.9952 | val_loss=0.7344 | val_acc=0.9928 | time=420.4s\nEpoch 037/100 | train_loss=0.7362 | train_acc=0.9917 | val_loss=0.7396 | val_acc=0.9907 | time=426.6s\nEpoch 038/100 | train_loss=0.7284 | train_acc=0.9954 | val_loss=0.7345 | val_acc=0.9929 | time=418.5s\nEpoch 039/100 | train_loss=0.7255 | train_acc=0.9960 | val_loss=0.7306 | val_acc=0.9936 | time=423.5s\nEpoch 040/100 | train_loss=0.7277 | train_acc=0.9965 | val_loss=0.7336 | val_acc=0.9943 | time=420.5s\nEpoch 041/100 | train_loss=0.7262 | train_acc=0.9957 | val_loss=0.7341 | val_acc=0.9925 | time=420.8s\nEpoch 042/100 | train_loss=0.7253 | train_acc=0.9961 | val_loss=0.7294 | val_acc=0.9943 | time=419.8s\nEpoch 043/100 | train_loss=0.7257 | train_acc=0.9955 | val_loss=0.7340 | val_acc=0.9926 | time=417.0s\nEpoch 044/100 | train_loss=0.7231 | train_acc=0.9963 | val_loss=0.7293 | val_acc=0.9936 | time=427.7s\nEpoch 045/100 | train_loss=0.7238 | train_acc=0.9964 | val_loss=0.7307 | val_acc=0.9936 | time=424.3s\nEpoch 046/100 | train_loss=0.7251 | train_acc=0.9957 | val_loss=0.7346 | val_acc=0.9923 | time=419.3s\nEpoch 047/100 | train_loss=0.7271 | train_acc=0.9948 | val_loss=0.7316 | val_acc=0.9933 | time=415.4s\nEpoch 048/100 | train_loss=0.7216 | train_acc=0.9971 | val_loss=0.7274 | val_acc=0.9950 | time=426.6s\nEpoch 049/100 | train_loss=0.7249 | train_acc=0.9962 | val_loss=0.7323 | val_acc=0.9938 | time=425.3s\nEpoch 050/100 | train_loss=0.7207 | train_acc=0.9968 | val_loss=0.7243 | val_acc=0.9959 | time=421.8s\nEpoch 051/100 | train_loss=0.7220 | train_acc=0.9972 | val_loss=0.7285 | val_acc=0.9953 | time=422.3s\nEpoch 052/100 | train_loss=0.7214 | train_acc=0.9974 | val_loss=0.7286 | val_acc=0.9947 | time=424.0s\nEpoch 053/100 | train_loss=0.7253 | train_acc=0.9950 | val_loss=0.7326 | val_acc=0.9923 | time=421.4s\nEpoch 054/100 | train_loss=0.7220 | train_acc=0.9969 | val_loss=0.7252 | val_acc=0.9961 | time=528.1s\nEpoch 055/100 | train_loss=0.7252 | train_acc=0.9962 | val_loss=0.7326 | val_acc=0.9932 | time=420.6s\nEpoch 056/100 | train_loss=0.7213 | train_acc=0.9971 | val_loss=0.7282 | val_acc=0.9952 | time=417.0s\nEpoch 057/100 | train_loss=0.7237 | train_acc=0.9958 | val_loss=0.7307 | val_acc=0.9937 | time=416.6s\nEpoch 058/100 | train_loss=0.7211 | train_acc=0.9967 | val_loss=0.7295 | val_acc=0.9926 | time=415.5s\nEpoch 059/100 | train_loss=0.7194 | train_acc=0.9971 | val_loss=0.7276 | val_acc=0.9935 | time=418.0s\nEpoch 060/100 | train_loss=0.7226 | train_acc=0.9966 | val_loss=0.7303 | val_acc=0.9938 | time=416.3s\nEpoch 061/100 | train_loss=0.7202 | train_acc=0.9964 | val_loss=0.7251 | val_acc=0.9950 | time=415.9s\nEpoch 062/100 | train_loss=0.7221 | train_acc=0.9958 | val_loss=0.7291 | val_acc=0.9935 | time=414.1s\nEpoch 063/100 | train_loss=0.7186 | train_acc=0.9975 | val_loss=0.7259 | val_acc=0.9946 | time=415.4s\nEpoch 064/100 | train_loss=0.7243 | train_acc=0.9955 | val_loss=0.7314 | val_acc=0.9933 | time=415.8s\nEpoch 065/100 | train_loss=0.7187 | train_acc=0.9970 | val_loss=0.7226 | val_acc=0.9958 | time=416.3s\nEpoch 066/100 | train_loss=0.7214 | train_acc=0.9967 | val_loss=0.7268 | val_acc=0.9951 | time=413.6s\nEpoch 067/100 | train_loss=0.7211 | train_acc=0.9963 | val_loss=0.7264 | val_acc=0.9947 | time=414.0s\nEpoch 068/100 | train_loss=0.7201 | train_acc=0.9965 | val_loss=0.7261 | val_acc=0.9945 | time=414.9s\nEpoch 069/100 | train_loss=0.7198 | train_acc=0.9975 | val_loss=0.7249 | val_acc=0.9958 | time=414.3s\nEpoch 070/100 | train_loss=0.7221 | train_acc=0.9960 | val_loss=0.7273 | val_acc=0.9941 | time=416.0s\nEpoch 071/100 | train_loss=0.7209 | train_acc=0.9973 | val_loss=0.7249 | val_acc=0.9958 | time=415.7s\nEpoch 072/100 | train_loss=0.7181 | train_acc=0.9980 | val_loss=0.7220 | val_acc=0.9964 | time=414.9s\nEpoch 073/100 | train_loss=0.7157 | train_acc=0.9984 | val_loss=0.7216 | val_acc=0.9961 | time=415.2s\nEpoch 074/100 | train_loss=0.7178 | train_acc=0.9974 | val_loss=0.7222 | val_acc=0.9964 | time=414.4s\nEpoch 075/100 | train_loss=0.7264 | train_acc=0.9946 | val_loss=0.7342 | val_acc=0.9916 | time=413.5s\nEpoch 076/100 | train_loss=0.7240 | train_acc=0.9959 | val_loss=0.7281 | val_acc=0.9944 | time=417.2s\nEpoch 077/100 | train_loss=0.7180 | train_acc=0.9978 | val_loss=0.7222 | val_acc=0.9966 | time=412.7s\nEpoch 078/100 | train_loss=0.7223 | train_acc=0.9968 | val_loss=0.7299 | val_acc=0.9944 | time=417.1s\nEpoch 079/100 | train_loss=0.7209 | train_acc=0.9967 | val_loss=0.7276 | val_acc=0.9942 | time=414.9s\nEpoch 080/100 | train_loss=0.7210 | train_acc=0.9964 | val_loss=0.7268 | val_acc=0.9946 | time=414.5s\nEpoch 081/100 | train_loss=0.7168 | train_acc=0.9977 | val_loss=0.7233 | val_acc=0.9955 | time=414.0s\nEpoch 082/100 | train_loss=0.7197 | train_acc=0.9968 | val_loss=0.7268 | val_acc=0.9940 | time=412.4s\nEpoch 083/100 | train_loss=0.7261 | train_acc=0.9953 | val_loss=0.7323 | val_acc=0.9935 | time=412.7s\nEpoch 084/100 | train_loss=0.7214 | train_acc=0.9961 | val_loss=0.7264 | val_acc=0.9939 | time=412.5s\nEpoch 085/100 | train_loss=0.7188 | train_acc=0.9972 | val_loss=0.7271 | val_acc=0.9942 | time=412.1s\nEpoch 086/100 | train_loss=0.7201 | train_acc=0.9977 | val_loss=0.7249 | val_acc=0.9962 | time=413.0s\nEpoch 087/100 | train_loss=0.7181 | train_acc=0.9973 | val_loss=0.7252 | val_acc=0.9945 | time=414.1s\nEpoch 088/100 | train_loss=0.7197 | train_acc=0.9968 | val_loss=0.7286 | val_acc=0.9928 | time=412.5s\nEpoch 089/100 | train_loss=0.7182 | train_acc=0.9970 | val_loss=0.7266 | val_acc=0.9936 | time=412.2s\nEpoch 090/100 | train_loss=0.7175 | train_acc=0.9977 | val_loss=0.7226 | val_acc=0.9962 | time=412.1s\nEpoch 091/100 | train_loss=0.7152 | train_acc=0.9982 | val_loss=0.7207 | val_acc=0.9963 | time=412.7s\nEpoch 092/100 | train_loss=0.7195 | train_acc=0.9977 | val_loss=0.7231 | val_acc=0.9962 | time=413.3s\nEpoch 093/100 | train_loss=0.7158 | train_acc=0.9977 | val_loss=0.7213 | val_acc=0.9958 | time=413.5s\nEpoch 094/100 | train_loss=0.7163 | train_acc=0.9978 | val_loss=0.7215 | val_acc=0.9965 | time=412.9s\nEpoch 095/100 | train_loss=0.7215 | train_acc=0.9977 | val_loss=0.7253 | val_acc=0.9964 | time=413.7s\nEpoch 096/100 | train_loss=0.7186 | train_acc=0.9972 | val_loss=0.7232 | val_acc=0.9960 | time=412.7s\nEpoch 097/100 | train_loss=0.7165 | train_acc=0.9978 | val_loss=0.7233 | val_acc=0.9947 | time=413.1s\nEpoch 098/100 | train_loss=0.7163 | train_acc=0.9981 | val_loss=0.7211 | val_acc=0.9961 | time=411.6s\nEpoch 099/100 | train_loss=0.7166 | train_acc=0.9983 | val_loss=0.7208 | val_acc=0.9970 | time=414.1s\nEpoch 100/100 | train_loss=0.7166 | train_acc=0.9979 | val_loss=0.7217 | val_acc=0.9960 | time=415.9s\n\nTotal training time: 698.24 minutes\n\nTest results: TestAcc=0.9966, Precision=0.9967, Recall=0.9966, F1=0.9966\n\nEfficiency summary:\n         Model  Params(M)  FLOPs(M)  Inference(ms)  Size(MB)  ValAcc(%)  TestAcc(%)  Precision  Recall    F1\nPlantaNet_ReLU       2.58    1213.9          23.48      9.85       99.7       99.66      0.997   0.997 0.997\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =========================\n# Cell 3 — Grad-CAM (runs separately)\n# =========================\n\nprint(\"\\nIdentifying correctly classified samples for Grad-CAM...\")\n\nif test_ds is None:\n    raise ValueError(\"No test dataset found. Put a test/ folder inside DATA_ROOT to run Grad-CAM.\")\n\ncorrect_samples = {cls: [] for cls in range(NUM_CLASSES)}\neval_model.eval()\n\nwith torch.no_grad():\n    for path, label in test_ds.samples:\n        img = Image.open(path).convert(\"RGB\")\n        tensor = eval_tfm(img).unsqueeze(0).to(DEVICE)\n        out = eval_model(tensor)\n        pred = out.argmax(1).item()\n        if pred == label:\n            correct_samples[label].append(path)\n\nprint(\"Done. Now generating Grad-CAM (one correctly classified image per class).\")\n\ngradcam_dir = os.path.join(OUT_DIR, \"gradcam\")\nos.makedirs(gradcam_dir, exist_ok=True)\n\n# Load weights safely\ngrad_model = PlantaNetReLU(NUM_CLASSES)\n\nbest_weights_path = os.path.join(OUT_DIR, \"PlantaNet_ReLU_best_weights.pth\")\n\nif os.path.exists(best_weights_path):\n    grad_model.load_state_dict(torch.load(best_weights_path, map_location=DEVICE), strict=False)\nelif best_state is not None:\n    grad_model.load_state_dict(best_state, strict=False)\nelse:\n    print(\"No best weights found — Grad-CAM will use current model weights.\")\n\ngrad_model = grad_model.to(DEVICE).eval()\ntarget_layer = grad_model.conv_extra[0]\n\n\nclass GradCAMFixed:\n    def __init__(self, model, target_layer):\n        self.model = model\n        self.target_layer = target_layer\n        self.gradients = None\n        self.activations = None\n\n        target_layer.register_forward_hook(self._save_activation)\n        target_layer.register_full_backward_hook(self._save_gradient)\n\n    def _save_activation(self, module, inp, out):\n        self.activations = out.detach()\n\n    def _save_gradient(self, module, grad_input, grad_output):\n        self.gradients = grad_output[0].detach()\n\n    def generate(self, input_tensor, class_idx=None):\n        self.model.zero_grad()\n        input_tensor = input_tensor.to(DEVICE)\n        outputs = self.model(input_tensor)\n\n        if class_idx is None:\n            class_idx = int(outputs.argmax(dim=1).item())\n\n        score = outputs[:, class_idx]\n        score.backward(retain_graph=False)\n\n        grads = self.gradients\n        acts = self.activations\n        weights = grads.mean(dim=(2, 3), keepdim=True)\n        cam = (weights * acts).sum(dim=1).squeeze(0)\n        cam = torch.relu(cam)\n\n        cam_np = cam.cpu().numpy()\n        cam_np -= cam_np.min()\n        cam_np /= (cam_np.max() + 1e-9)\n        return cam_np\n\n\ngradcam = GradCAMFixed(grad_model, target_layer)\n\nmean = np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\nstd = np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1)\n\ndef denorm_tensor(img_tensor):\n    arr = img_tensor.cpu().numpy()\n    arr = (arr * std) + mean\n    arr = np.clip(arr, 0, 1)\n    return np.transpose(arr, (1, 2, 0))\n\n\nfor cls in range(NUM_CLASSES):\n    if len(correct_samples[cls]) == 0:\n        print(f\"No correctly classified sample for class {class_names[cls]}\")\n        continue\n\n    path = correct_samples[cls][0]\n    try:\n        pil = Image.open(path).convert(\"RGB\")\n        tensor = eval_tfm(pil).unsqueeze(0).to(DEVICE)\n        cam_map = gradcam.generate(tensor, class_idx=cls)\n    except Exception as e:\n        print(f\"Grad-CAM error for {path}: {e}\")\n        continue\n\n    cam_resized = cv2.resize(cam_map, (IMG_SIZE, IMG_SIZE))\n    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)[:, :, ::-1]\n\n    original = denorm_tensor(tensor[0])\n    orig_uint8 = np.uint8(original * 255)\n    heat_uint8 = np.uint8(heatmap)\n    overlay = cv2.addWeighted(orig_uint8, 0.6, heat_uint8, 0.4, 0)\n\n    cls_name = class_names[cls].replace(\"/\", \"_\").replace(\" \", \"_\")\n    cls_folder = os.path.join(gradcam_dir, cls_name)\n    os.makedirs(cls_folder, exist_ok=True)\n    save_path = os.path.join(cls_folder, f\"gradcam_{cls_name}.png\")\n\n    cv2.imwrite(save_path, overlay[:, :, ::-1])\n    print(\"Saved Grad-CAM for class:\", cls_name)\n\nprint(\"\\nGrad-CAM generation completed. Saved under:\", gradcam_dir)\nprint(\"\\nFinished\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:41:11.916403Z","iopub.execute_input":"2025-12-02T01:41:11.916703Z","iopub.status.idle":"2025-12-02T01:42:41.745052Z","shell.execute_reply.started":"2025-12-02T01:41:11.916673Z","shell.execute_reply":"2025-12-02T01:42:41.744317Z"}},"outputs":[{"name":"stdout","text":"\nIdentifying correctly classified samples for Grad-CAM...\nDone. Now generating Grad-CAM (one correctly classified image per class).\nSaved Grad-CAM for class: Apple___Apple_scab\nSaved Grad-CAM for class: Apple___Black_rot\nSaved Grad-CAM for class: Apple___Cedar_apple_rust\nSaved Grad-CAM for class: Apple___healthy\nSaved Grad-CAM for class: Banana___cordana\nSaved Grad-CAM for class: Banana___healthy\nSaved Grad-CAM for class: Banana___pestalotiopsis\nSaved Grad-CAM for class: Banana___sigatoka\nSaved Grad-CAM for class: Bean___angular_leaf_spot\nSaved Grad-CAM for class: Bean___bean_rust\nSaved Grad-CAM for class: Bean___healthy\nSaved Grad-CAM for class: Blueberry___healthy\nSaved Grad-CAM for class: Corn___Cercospora_leaf_spot_Gray_leaf_spot\nSaved Grad-CAM for class: Corn___Common_rust_\nSaved Grad-CAM for class: Corn___Northern_Leaf_Blight\nSaved Grad-CAM for class: Corn___healthy\nSaved Grad-CAM for class: Grape___Black_rot\nSaved Grad-CAM for class: Grape___Esca_(Black_Measles)\nSaved Grad-CAM for class: Grape___Leaf_blight_(Isariopsis_Leaf_Spot)\nSaved Grad-CAM for class: Grape___healthy\nSaved Grad-CAM for class: Mango___Anthracnose\nSaved Grad-CAM for class: Mango___Bacterial_Canker\nSaved Grad-CAM for class: Mango___Cutting_Weevil\nSaved Grad-CAM for class: Mango___Die_Back\nSaved Grad-CAM for class: Mango___Gall_Midge\nSaved Grad-CAM for class: Mango___Healthy\nSaved Grad-CAM for class: Mango___Powdery_Mildew\nSaved Grad-CAM for class: Mango___Sooty_Mould\nSaved Grad-CAM for class: Pepper,_bell___Bacterial_spot\nSaved Grad-CAM for class: Pepper,_bell___healthy\nSaved Grad-CAM for class: Potato___Early_blight\nSaved Grad-CAM for class: Potato___Late_blight\nSaved Grad-CAM for class: Potato___healthy\nSaved Grad-CAM for class: Rice___bacterial_leaf_blight\nSaved Grad-CAM for class: Rice___brown_spot\nSaved Grad-CAM for class: Rice___healthy\nSaved Grad-CAM for class: Rice___leaf_blast\nSaved Grad-CAM for class: Rice___leaf_scald\nSaved Grad-CAM for class: Rice___narrow_brown_spot\nSaved Grad-CAM for class: Strawberry___Leaf_scorch\nSaved Grad-CAM for class: Strawberry___healthy\nSaved Grad-CAM for class: Tomato___Bacterial_spot\nSaved Grad-CAM for class: Tomato___Early_blight\nSaved Grad-CAM for class: Tomato___Late_blight\nSaved Grad-CAM for class: Tomato___Leaf_Mold\nSaved Grad-CAM for class: Tomato___Septoria_leaf_spot\nSaved Grad-CAM for class: Tomato___Spider_mites_Two-spotted_spider_mite\nSaved Grad-CAM for class: Tomato___Target_Spot\nSaved Grad-CAM for class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\nSaved Grad-CAM for class: Tomato___Tomato_mosaic_virus\nSaved Grad-CAM for class: Tomato___healthy\n\nGrad-CAM generation completed. Saved under: /kaggle/working/plantanet_relu_final/gradcam\n\nFinished\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os, shutil\n\n# Folder where Grad-CAM images are saved\ngradcam_dir = os.path.join(OUT_DIR, \"gradcam\")   # or \"gradcam_pp\" for Grad-CAM++\n\n# Output zip path\nzip_path = os.path.join(OUT_DIR, \"gradcam_results.zip\")\n\n# Remove old zip if it exists\nif os.path.exists(zip_path):\n    os.remove(zip_path)\n\n# Make zip\nshutil.make_archive(\n    base_name=zip_path.replace(\".zip\",\"\"),\n    format=\"zip\",\n    root_dir=gradcam_dir\n)\n\nprint(\"✅ Grad-CAM zipped at:\", zip_path)\nprint(\"Download it from Kaggle → Output tab / Files panel.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T01:47:17.993375Z","iopub.execute_input":"2025-12-02T01:47:17.994179Z","iopub.status.idle":"2025-12-02T01:47:18.070535Z","shell.execute_reply.started":"2025-12-02T01:47:17.994148Z","shell.execute_reply":"2025-12-02T01:47:18.069948Z"}},"outputs":[{"name":"stdout","text":"✅ Grad-CAM zipped at: /kaggle/working/plantanet_relu_final/gradcam_results.zip\nDownload it from Kaggle → Output tab / Files panel.\n","output_type":"stream"}],"execution_count":5}]}