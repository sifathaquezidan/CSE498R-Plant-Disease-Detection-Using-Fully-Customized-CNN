{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13166744,"sourceType":"datasetVersion","datasetId":8343159}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install thop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T08:42:31.574574Z","iopub.execute_input":"2025-11-14T08:42:31.575142Z","iopub.status.idle":"2025-11-14T08:43:54.624082Z","shell.execute_reply.started":"2025-11-14T08:42:31.575115Z","shell.execute_reply":"2025-11-14T08:43:54.623215Z"}},"outputs":[{"name":"stdout","text":"Collecting thop\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->thop)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->thop)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->thop)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.3)\nDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# LeafNet-ReLU(Fully Customized) Training \n# Author: Md. Sifat Haque Zidan\n\nimport os, time, copy, random, numpy as np, pandas as pd\nimport torch, torch.nn as nn, torch.optim as optim\nimport matplotlib.pyplot as plt, seaborn as sns\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support\n\n# FLOPs\ntry:\n    from thop import profile\n    THOP_AVAILABLE = True\nexcept Exception:\n    THOP_AVAILABLE = False\n    print(\"thop not available — FLOPs skipped\")\n\n# Settings \nDATA_ROOT = \"/kaggle/input/plant-disease-dataset/Dataset_Final_V2_Split\"\nOUT_DIR = \"/kaggle/working/leafnet_relu_final\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nIMG_SIZE, BATCH_SIZE, EPOCHS = 160, 32, 100\nLR, NUM_WORKERS, SEED = 1e-3, 4, 42\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\nif torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\nprint(f\"Device: {DEVICE} | GPUs: {torch.cuda.device_count()}\")\n\n# Dataset \ntfm = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n])\ntrain_ds = datasets.ImageFolder(f\"{DATA_ROOT}/train\", transform=tfm)\nval_ds   = datasets.ImageFolder(f\"{DATA_ROOT}/val\", transform=tfm)\ntest_ds  = datasets.ImageFolder(f\"{DATA_ROOT}/test\", transform=tfm)\ntrain_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nval_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\ntest_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\nclass_names = train_ds.classes; NUM_CLASSES = len(class_names)\nprint(f\"Classes={NUM_CLASSES}, Train={len(train_ds)}, Val={len(val_ds)}, Test={len(test_ds)}\")\n\n# Model \nclass DepthwiseSeparableConv(nn.Module):\n    def __init__(self, in_ch, out_ch, stride=1):\n        super().__init__()\n        self.depth = nn.Conv2d(in_ch, in_ch, 3, stride, 1, groups=in_ch, bias=False)\n        self.point = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n        self.norm = nn.GroupNorm(8 if out_ch%8==0 else 4, out_ch)\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x): return self.act(self.norm(self.point(self.depth(x))))\n\nclass LeafNetReLU(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        c1,c2,c3,c4 = 144,224,320,448\n        self.stem = nn.Sequential(nn.Conv2d(3,c1,3,1,1,bias=False),\n                                  nn.GroupNorm(8,c1), nn.ReLU(inplace=True))\n        self.block1 = nn.Sequential(DepthwiseSeparableConv(c1,c2,2), nn.Dropout(0.15))\n        self.block2 = nn.Sequential(DepthwiseSeparableConv(c2,c3,2), nn.Dropout(0.20))\n        self.block3 = nn.Sequential(DepthwiseSeparableConv(c3,c4,2), nn.Dropout(0.25))\n        self.conv_extra = nn.Sequential(nn.Conv2d(c4,c4,3,1,1,bias=False),\n                                        nn.GroupNorm(8,c4), nn.ReLU(inplace=True), nn.Dropout(0.25))\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Sequential(nn.Flatten(),\n                                        nn.Linear(c4,1024,bias=False),\n                                        nn.GroupNorm(8,1024),\n                                        nn.ReLU(inplace=True),\n                                        nn.Dropout(0.4),\n                                        nn.Linear(1024,num_classes))\n    def forward(self,x):\n        x=self.stem(x); x=self.block1(x); x=self.block2(x); x=self.block3(x)\n        x=self.conv_extra(x); x=self.gap(x); x=self.classifier(x); return x\n\nmodel = LeafNetReLU(NUM_CLASSES)\nparams = sum(p.numel() for p in model.parameters())\nprint(f\"Total Params: {params:,} ({params/1e6:.3f}M)\")\n\n# Efficiency Summary \nflops_M, inf_ms, size_mb = None, None, None\nif THOP_AVAILABLE:\n    f, _ = profile(model, inputs=(torch.randn(1,3,IMG_SIZE,IMG_SIZE),), verbose=False)\n    flops_M = f/1e6\ndummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE).to(DEVICE)\nmodel = model.to(DEVICE)\nif torch.cuda.device_count()>1:\n    model = nn.DataParallel(model)\nmodel.eval(); torch.cuda.synchronize() if DEVICE.type=='cuda' else None\nstart=time.time(); \nwith torch.no_grad():\n    for _ in range(30): _=model(dummy)\ntorch.cuda.synchronize() if DEVICE.type=='cuda' else None\ninf_ms=(time.time()-start)/30*1000\ntmp=os.path.join(OUT_DIR,\"tmp.pth\"); torch.save(model.state_dict(),tmp)\nsize_mb=os.path.getsize(tmp)/(1024*1024); os.remove(tmp)\n\nprint(f\"FLOPs={flops_M:.2f}M | Inference={inf_ms:.2f}ms | Size={size_mb:.2f}MB\")\n\n# Training \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adamax(model.parameters(), lr=LR)\n\ndef run_epoch(loader, train=True):\n    if train: model.train()\n    else: model.eval()\n    total_loss, correct, total = 0,0,0\n    preds_all, labels_all = [], []\n    start=time.time()\n    with torch.set_grad_enabled(train):\n        for x,y in loader:\n            x,y=x.to(DEVICE),y.to(DEVICE)\n            if train: optimizer.zero_grad()\n            out=model(x); loss=criterion(out,y)\n            if train: loss.backward(); optimizer.step()\n            _,p=torch.max(out,1)\n            total_loss+=loss.item()*x.size(0)\n            correct+=torch.sum(p==y).item(); total+=x.size(0)\n            preds_all.append(p.cpu().numpy()); labels_all.append(y.cpu().numpy())\n    t=time.time()-start\n    preds_all=np.concatenate(preds_all); labels_all=np.concatenate(labels_all)\n    return total_loss/total, correct/total, preds_all, labels_all, t\n\nhistory=[]; best_val_acc=0; best_state=None\nfor e in range(1,EPOCHS+1):\n    tr_l,tr_a,_,_,tr_t=run_epoch(train_loader,True)\n    vl_l,vl_a,_,_,vl_t=run_epoch(val_loader,False)\n    ep_t=tr_t+vl_t\n    if vl_a>best_val_acc:\n        best_val_acc=vl_a; best_state=copy.deepcopy(model.state_dict())\n        torch.save(best_state, f\"{OUT_DIR}/LeafNet_ReLU_best.pth\")\n    history.append([e,tr_l,tr_a,vl_l,vl_a,tr_t,vl_t,ep_t])\n    print(f\"Epoch {e:02d}/{EPOCHS} | Train {tr_a:.4f} | Val {vl_a:.4f} | Loss {tr_l:.3f}/{vl_l:.3f} | Time {ep_t:.1f}s\")\n\nhist_df=pd.DataFrame(history,columns=[\"epoch\",\"train_loss\",\"train_acc\",\"val_loss\",\"val_acc\",\"train_time\",\"val_time\",\"epoch_time\"])\nhist_df.to_csv(f\"{OUT_DIR}/LeafNet_ReLU_history.csv\",index=False)\n\n# Test Evaluation \nif best_state: model.load_state_dict(best_state)\nmodel.eval(); y_t,y_p=[],[]\nwith torch.no_grad():\n    for x,y in test_loader:\n        x,y=x.to(DEVICE),y.to(DEVICE)\n        out=model(x); _,p=torch.max(out,1)\n        y_t.append(y.cpu().numpy()); y_p.append(p.cpu().numpy())\ny_t=np.concatenate(y_t); y_p=np.concatenate(y_p)\nacc=np.mean(y_t==y_p)\nprec,rec,f1,_=precision_recall_fscore_support(y_t,y_p,average=\"weighted\")\nprint(f\"\\nPrecision={prec:.4f}, Recall={rec:.4f}, F1={f1:.4f}, TestAcc={acc:.4f}\")\npd.DataFrame(classification_report(y_t,y_p,target_names=class_names,output_dict=True)).T.to_csv(f\"{OUT_DIR}/classification_report.csv\")\n\n# Visualization \nplt.figure(figsize=(8,5))\nplt.plot(hist_df[\"epoch\"],hist_df[\"train_loss\"],label=\"Train Loss\")\nplt.plot(hist_df[\"epoch\"],hist_df[\"val_loss\"],label=\"Val Loss\")\nplt.xlabel(\"Epoch\");plt.ylabel(\"Loss\");plt.legend();plt.title(\"Loss Curve\")\nplt.tight_layout();plt.savefig(f\"{OUT_DIR}/loss_curve.png\",dpi=150);plt.close()\n\nplt.figure(figsize=(8,5))\nplt.plot(hist_df[\"epoch\"],hist_df[\"train_acc\"],label=\"Train Acc\")\nplt.plot(hist_df[\"epoch\"],hist_df[\"val_acc\"],label=\"Val Acc\")\nplt.xlabel(\"Epoch\");plt.ylabel(\"Accuracy\");plt.legend();plt.title(\"Accuracy Curve\")\nplt.tight_layout();plt.savefig(f\"{OUT_DIR}/accuracy_curve.png\",dpi=150);plt.close()\n\ncm=confusion_matrix(y_t,y_p)\nplt.figure(figsize=(12,10))\nsns.heatmap(cm,annot=False,cmap=\"Blues\")\nplt.title(\"Confusion Matrix - LeafNet_ReLU\")\nplt.tight_layout();plt.savefig(f\"{OUT_DIR}/confusion_matrix.png\",dpi=150);plt.close()\n\nplt.figure(figsize=(6,4))\nplt.bar([\"Precision\",\"Recall\",\"F1\",\"Accuracy\"],[prec,rec,f1,acc],color=[\"#4CAF50\",\"#2196F3\",\"#FFC107\",\"#9C27B0\"])\nplt.ylim(0,1);plt.title(\"LeafNet_ReLU Metrics\");plt.tight_layout()\nplt.savefig(f\"{OUT_DIR}/metrics_barplot.png\",dpi=150);plt.close()\n\n# Efficiency Table \nsummary = pd.DataFrame([{\n    \"Model\":\"LeafNet_ReLU\",\n    \"Params(M)\":round(params/1e6,3),\n    \"FLOPs(M)\":round(flops_M,2) if flops_M else None,\n    \"Inference(ms)\":round(inf_ms,2),\n    \"Size(MB)\":round(size_mb,2),\n    \"ValAcc(%)\":round(best_val_acc*100,2),\n    \"TestAcc(%)\":round(acc*100,2),\n    \"Precision\":round(prec,3),\n    \"Recall\":round(rec,3),\n    \"F1\":round(f1,3)\n}])\nsummary.to_csv(f\"{OUT_DIR}/LeafNet_ReLU_efficiency_summary.csv\",index=False)\nprint(\"\\n Efficiency Summary\")\nprint(summary.to_string(index=False))\n\nprint(\"\\n All results saved under:\", OUT_DIR)\nfor f in sorted(os.listdir(OUT_DIR)): print(\" -\", f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T08:44:46.552951Z","iopub.execute_input":"2025-11-14T08:44:46.553328Z","iopub.status.idle":"2025-11-14T15:48:37.343580Z","shell.execute_reply.started":"2025-11-14T08:44:46.553297Z","shell.execute_reply":"2025-11-14T15:48:37.342424Z"}},"outputs":[{"name":"stdout","text":"Device: cuda | GPUs: 2\nClasses=51, Train=95504, Val=20472, Test=20506\nTotal Params: 2,579,955 (2.580M)\nFLOPs=1213.90M | Inference=34.37ms | Size=9.86MB\nEpoch 01/100 | Train 0.5989 | Val 0.7929 | Loss 1.344/0.630 | Time 340.8s\nEpoch 02/100 | Train 0.8408 | Val 0.8624 | Loss 0.485/0.413 | Time 255.3s\nEpoch 03/100 | Train 0.8926 | Val 0.9246 | Loss 0.318/0.223 | Time 248.2s\nEpoch 04/100 | Train 0.9208 | Val 0.9375 | Loss 0.236/0.177 | Time 248.3s\nEpoch 05/100 | Train 0.9378 | Val 0.9596 | Loss 0.182/0.120 | Time 249.6s\nEpoch 06/100 | Train 0.9497 | Val 0.9618 | Loss 0.147/0.111 | Time 249.4s\nEpoch 07/100 | Train 0.9579 | Val 0.9618 | Loss 0.122/0.109 | Time 250.6s\nEpoch 08/100 | Train 0.9640 | Val 0.9725 | Loss 0.104/0.079 | Time 249.5s\nEpoch 09/100 | Train 0.9673 | Val 0.9642 | Loss 0.091/0.104 | Time 250.2s\nEpoch 10/100 | Train 0.9724 | Val 0.9747 | Loss 0.079/0.070 | Time 248.5s\nEpoch 11/100 | Train 0.9753 | Val 0.9748 | Loss 0.071/0.074 | Time 248.1s\nEpoch 12/100 | Train 0.9781 | Val 0.9828 | Loss 0.064/0.054 | Time 248.0s\nEpoch 13/100 | Train 0.9798 | Val 0.9785 | Loss 0.057/0.062 | Time 249.8s\nEpoch 14/100 | Train 0.9822 | Val 0.9846 | Loss 0.052/0.047 | Time 248.8s\nEpoch 15/100 | Train 0.9832 | Val 0.9835 | Loss 0.049/0.048 | Time 248.2s\nEpoch 16/100 | Train 0.9843 | Val 0.9852 | Loss 0.044/0.041 | Time 249.7s\nEpoch 17/100 | Train 0.9851 | Val 0.9823 | Loss 0.042/0.054 | Time 249.1s\nEpoch 18/100 | Train 0.9866 | Val 0.9827 | Loss 0.037/0.055 | Time 248.7s\nEpoch 19/100 | Train 0.9875 | Val 0.9873 | Loss 0.035/0.041 | Time 249.9s\nEpoch 20/100 | Train 0.9892 | Val 0.9879 | Loss 0.031/0.037 | Time 248.3s\nEpoch 21/100 | Train 0.9897 | Val 0.9874 | Loss 0.030/0.038 | Time 252.6s\nEpoch 22/100 | Train 0.9905 | Val 0.9889 | Loss 0.027/0.033 | Time 249.4s\nEpoch 23/100 | Train 0.9917 | Val 0.9880 | Loss 0.024/0.039 | Time 252.0s\nEpoch 24/100 | Train 0.9907 | Val 0.9852 | Loss 0.026/0.043 | Time 251.0s\nEpoch 25/100 | Train 0.9918 | Val 0.9905 | Loss 0.023/0.032 | Time 252.7s\nEpoch 26/100 | Train 0.9922 | Val 0.9900 | Loss 0.022/0.034 | Time 252.9s\nEpoch 27/100 | Train 0.9928 | Val 0.9904 | Loss 0.020/0.032 | Time 253.0s\nEpoch 28/100 | Train 0.9931 | Val 0.9864 | Loss 0.019/0.042 | Time 252.1s\nEpoch 29/100 | Train 0.9932 | Val 0.9902 | Loss 0.019/0.032 | Time 251.8s\nEpoch 30/100 | Train 0.9938 | Val 0.9906 | Loss 0.018/0.029 | Time 252.7s\nEpoch 31/100 | Train 0.9936 | Val 0.9906 | Loss 0.018/0.034 | Time 253.3s\nEpoch 32/100 | Train 0.9947 | Val 0.9873 | Loss 0.016/0.042 | Time 249.8s\nEpoch 33/100 | Train 0.9951 | Val 0.9887 | Loss 0.014/0.041 | Time 249.6s\nEpoch 34/100 | Train 0.9952 | Val 0.9897 | Loss 0.014/0.035 | Time 252.5s\nEpoch 35/100 | Train 0.9952 | Val 0.9917 | Loss 0.013/0.028 | Time 249.5s\nEpoch 36/100 | Train 0.9955 | Val 0.9911 | Loss 0.013/0.030 | Time 252.4s\nEpoch 37/100 | Train 0.9958 | Val 0.9893 | Loss 0.013/0.035 | Time 250.9s\nEpoch 38/100 | Train 0.9962 | Val 0.9908 | Loss 0.011/0.029 | Time 250.1s\nEpoch 39/100 | Train 0.9958 | Val 0.9894 | Loss 0.012/0.031 | Time 248.4s\nEpoch 40/100 | Train 0.9963 | Val 0.9928 | Loss 0.010/0.025 | Time 248.3s\nEpoch 41/100 | Train 0.9964 | Val 0.9914 | Loss 0.011/0.032 | Time 251.4s\nEpoch 42/100 | Train 0.9965 | Val 0.9916 | Loss 0.010/0.029 | Time 251.1s\nEpoch 43/100 | Train 0.9960 | Val 0.9921 | Loss 0.011/0.028 | Time 251.4s\nEpoch 44/100 | Train 0.9966 | Val 0.9933 | Loss 0.010/0.024 | Time 251.8s\nEpoch 45/100 | Train 0.9966 | Val 0.9929 | Loss 0.009/0.025 | Time 249.1s\nEpoch 46/100 | Train 0.9966 | Val 0.9935 | Loss 0.009/0.024 | Time 250.7s\nEpoch 47/100 | Train 0.9970 | Val 0.9931 | Loss 0.008/0.026 | Time 252.0s\nEpoch 48/100 | Train 0.9972 | Val 0.9931 | Loss 0.008/0.027 | Time 247.8s\nEpoch 49/100 | Train 0.9973 | Val 0.9941 | Loss 0.009/0.020 | Time 246.7s\nEpoch 50/100 | Train 0.9974 | Val 0.9927 | Loss 0.008/0.025 | Time 247.5s\nEpoch 51/100 | Train 0.9972 | Val 0.9939 | Loss 0.008/0.021 | Time 248.8s\nEpoch 52/100 | Train 0.9972 | Val 0.9939 | Loss 0.008/0.023 | Time 249.2s\nEpoch 53/100 | Train 0.9975 | Val 0.9919 | Loss 0.007/0.028 | Time 247.7s\nEpoch 54/100 | Train 0.9975 | Val 0.9933 | Loss 0.007/0.025 | Time 247.7s\nEpoch 55/100 | Train 0.9975 | Val 0.9953 | Loss 0.007/0.019 | Time 248.2s\nEpoch 56/100 | Train 0.9974 | Val 0.9923 | Loss 0.008/0.031 | Time 245.7s\nEpoch 57/100 | Train 0.9977 | Val 0.9941 | Loss 0.007/0.024 | Time 246.3s\nEpoch 58/100 | Train 0.9976 | Val 0.9937 | Loss 0.007/0.025 | Time 246.5s\nEpoch 59/100 | Train 0.9976 | Val 0.9947 | Loss 0.006/0.019 | Time 248.2s\nEpoch 60/100 | Train 0.9975 | Val 0.9943 | Loss 0.007/0.022 | Time 249.6s\nEpoch 61/100 | Train 0.9980 | Val 0.9948 | Loss 0.005/0.022 | Time 250.9s\nEpoch 62/100 | Train 0.9979 | Val 0.9922 | Loss 0.007/0.030 | Time 251.6s\nEpoch 63/100 | Train 0.9983 | Val 0.9958 | Loss 0.005/0.018 | Time 256.4s\nEpoch 64/100 | Train 0.9974 | Val 0.9936 | Loss 0.007/0.025 | Time 252.4s\nEpoch 65/100 | Train 0.9979 | Val 0.9936 | Loss 0.006/0.025 | Time 250.2s\nEpoch 66/100 | Train 0.9983 | Val 0.9936 | Loss 0.005/0.030 | Time 246.3s\nEpoch 67/100 | Train 0.9982 | Val 0.9943 | Loss 0.005/0.023 | Time 250.0s\nEpoch 68/100 | Train 0.9983 | Val 0.9914 | Loss 0.005/0.031 | Time 250.3s\nEpoch 69/100 | Train 0.9979 | Val 0.9948 | Loss 0.006/0.023 | Time 249.9s\nEpoch 70/100 | Train 0.9987 | Val 0.9936 | Loss 0.004/0.026 | Time 250.4s\nEpoch 71/100 | Train 0.9985 | Val 0.9946 | Loss 0.005/0.024 | Time 248.9s\nEpoch 72/100 | Train 0.9983 | Val 0.9933 | Loss 0.005/0.028 | Time 250.7s\nEpoch 73/100 | Train 0.9984 | Val 0.9949 | Loss 0.005/0.021 | Time 247.0s\nEpoch 74/100 | Train 0.9982 | Val 0.9941 | Loss 0.005/0.025 | Time 248.5s\nEpoch 75/100 | Train 0.9985 | Val 0.9939 | Loss 0.004/0.025 | Time 248.2s\nEpoch 76/100 | Train 0.9982 | Val 0.9944 | Loss 0.005/0.024 | Time 251.2s\nEpoch 77/100 | Train 0.9986 | Val 0.9941 | Loss 0.005/0.026 | Time 251.6s\nEpoch 78/100 | Train 0.9985 | Val 0.9944 | Loss 0.004/0.023 | Time 249.4s\nEpoch 79/100 | Train 0.9987 | Val 0.9952 | Loss 0.004/0.024 | Time 250.0s\nEpoch 80/100 | Train 0.9989 | Val 0.9947 | Loss 0.003/0.022 | Time 250.4s\nEpoch 81/100 | Train 0.9985 | Val 0.9941 | Loss 0.004/0.026 | Time 248.1s\nEpoch 82/100 | Train 0.9982 | Val 0.9953 | Loss 0.005/0.022 | Time 250.2s\nEpoch 83/100 | Train 0.9985 | Val 0.9942 | Loss 0.004/0.027 | Time 269.3s\nEpoch 84/100 | Train 0.9985 | Val 0.9937 | Loss 0.005/0.028 | Time 253.7s\nEpoch 85/100 | Train 0.9986 | Val 0.9948 | Loss 0.004/0.023 | Time 252.8s\nEpoch 86/100 | Train 0.9987 | Val 0.9948 | Loss 0.004/0.025 | Time 252.0s\nEpoch 87/100 | Train 0.9989 | Val 0.9955 | Loss 0.003/0.021 | Time 251.4s\nEpoch 88/100 | Train 0.9985 | Val 0.9945 | Loss 0.004/0.024 | Time 250.1s\nEpoch 89/100 | Train 0.9983 | Val 0.9942 | Loss 0.005/0.024 | Time 247.8s\nEpoch 90/100 | Train 0.9988 | Val 0.9953 | Loss 0.003/0.023 | Time 248.2s\nEpoch 91/100 | Train 0.9990 | Val 0.9958 | Loss 0.004/0.022 | Time 248.3s\nEpoch 92/100 | Train 0.9986 | Val 0.9950 | Loss 0.004/0.025 | Time 250.2s\nEpoch 93/100 | Train 0.9988 | Val 0.9951 | Loss 0.004/0.019 | Time 250.0s\nEpoch 94/100 | Train 0.9984 | Val 0.9955 | Loss 0.005/0.020 | Time 251.2s\nEpoch 95/100 | Train 0.9990 | Val 0.9954 | Loss 0.004/0.022 | Time 250.6s\nEpoch 96/100 | Train 0.9990 | Val 0.9961 | Loss 0.003/0.019 | Time 251.2s\nEpoch 97/100 | Train 0.9988 | Val 0.9950 | Loss 0.004/0.021 | Time 250.6s\nEpoch 98/100 | Train 0.9989 | Val 0.9940 | Loss 0.003/0.025 | Time 250.4s\nEpoch 99/100 | Train 0.9989 | Val 0.9955 | Loss 0.003/0.021 | Time 248.0s\nEpoch 100/100 | Train 0.9988 | Val 0.9914 | Loss 0.004/0.040 | Time 248.4s\n\nPrecision=0.9961, Recall=0.9960, F1=0.9961, TestAcc=0.9960\n\n Efficiency Summary\n       Model  Params(M)  FLOPs(M)  Inference(ms)  Size(MB)  ValAcc(%)  TestAcc(%)  Precision  Recall    F1\nLeafNet_ReLU       2.58    1213.9          34.37      9.86      99.61        99.6      0.996   0.996 0.996\n\n All results saved under: /kaggle/working/leafnet_relu_final\n - LeafNet_ReLU_best.pth\n - LeafNet_ReLU_efficiency_summary.csv\n - LeafNet_ReLU_history.csv\n - accuracy_curve.png\n - classification_report.csv\n - confusion_matrix.png\n - loss_curve.png\n - metrics_barplot.png\n","output_type":"stream"}],"execution_count":3}]}